{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fraud Detection from Credit Card History, Machine learning algorithms\n",
    "The following jupyter notebook contains a binary classification for fraud detection, from a credit card history. For this; we will explore the following three machine learning algorithms:\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* Linear Support Vector Machine\n",
    "\n",
    "This jupyter notebook would showcase the following:\n",
    "1. Confusion matrix for each of the models\n",
    "2. Cross validation metrics (precision, recall, f1_score, accuracy_score).\n",
    "3. Plot of probability distributions between real test data vs each models predictions."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:42:49.951991Z",
     "start_time": "2024-12-06T12:42:49.069781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the root project path\n",
    "root_project_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "#Append it to sys\n",
    "sys.path.append(root_project_path)\n",
    "\n",
    "#Import the necessary modules\n",
    "from utils import DataLoader, CreditCardPreprocesser"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marqu\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:42:57.291223Z",
     "start_time": "2024-12-06T12:42:54.888811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Set the folder name and data folder\n",
    "folder_name = \"data\"\n",
    "data_holder_path = os.path.join(os.getcwd(), os.pardir)\n",
    "\n",
    "#Set the folder name\n",
    "data_loader = DataLoader(data_folder_name=folder_name,\n",
    "    data_folder_path=data_holder_path)\n",
    "\n",
    "#Get the data\n",
    "df_data = data_loader.get_dataset()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the preprocessed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:05.067960Z",
     "start_time": "2024-12-06T12:42:58.505168Z"
    }
   },
   "source": [
    "#Create an instance of the Credit card processer\n",
    "credit_card_processer = CreditCardPreprocesser(df_data=df_data)\n",
    "\n",
    "#Obtain the df_preprocessed\n",
    "df_preprocessed = credit_card_processer.fetch_preprocessed_dataframe()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:55:27.799140Z",
     "start_time": "2024-12-06T11:55:27.785141Z"
    }
   },
   "cell_type": "code",
   "source": "df_preprocessed.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296675, 96)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:55:38.024051Z",
     "start_time": "2024-12-06T11:55:37.892640Z"
    }
   },
   "cell_type": "code",
   "source": "df_preprocessed.info()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 96 columns):\n",
      " #   Column                   Non-Null Count    Dtype  \n",
      "---  ------                   --------------    -----  \n",
      " 0   cc_num                   1296675 non-null  int64  \n",
      " 1   amt                      1296675 non-null  float64\n",
      " 2   gender                   1296675 non-null  int64  \n",
      " 3   zip                      1296675 non-null  int64  \n",
      " 4   lat                      1296675 non-null  float64\n",
      " 5   long                     1296675 non-null  float64\n",
      " 6   city_pop                 1296675 non-null  int64  \n",
      " 7   unix_time                1296675 non-null  int64  \n",
      " 8   merch_lat                1296675 non-null  float64\n",
      " 9   merch_long               1296675 non-null  float64\n",
      " 10  is_fraud                 1296675 non-null  int64  \n",
      " 11  merch_zipcode            1296675 non-null  float64\n",
      " 12  transaction_year         1296675 non-null  int32  \n",
      " 13  transaction_month        1296675 non-null  int32  \n",
      " 14  transaction_day          1296675 non-null  int32  \n",
      " 15  transaction_hour         1296675 non-null  int32  \n",
      " 16  transaction_minute       1296675 non-null  int32  \n",
      " 17  transaction_second       1296675 non-null  int32  \n",
      " 18  birth_year               1296675 non-null  int32  \n",
      " 19  birth_month              1296675 non-null  int32  \n",
      " 20  birth_day                1296675 non-null  int32  \n",
      " 21  category_food_dining     1296675 non-null  int64  \n",
      " 22  category_gas_transport   1296675 non-null  int64  \n",
      " 23  category_grocery_net     1296675 non-null  int64  \n",
      " 24  category_grocery_pos     1296675 non-null  int64  \n",
      " 25  category_health_fitness  1296675 non-null  int64  \n",
      " 26  category_home            1296675 non-null  int64  \n",
      " 27  category_kids_pets       1296675 non-null  int64  \n",
      " 28  category_misc_net        1296675 non-null  int64  \n",
      " 29  category_misc_pos        1296675 non-null  int64  \n",
      " 30  category_personal_care   1296675 non-null  int64  \n",
      " 31  category_shopping_net    1296675 non-null  int64  \n",
      " 32  category_shopping_pos    1296675 non-null  int64  \n",
      " 33  category_travel          1296675 non-null  int64  \n",
      " 34  state_AL                 1296675 non-null  int64  \n",
      " 35  state_AR                 1296675 non-null  int64  \n",
      " 36  state_AZ                 1296675 non-null  int64  \n",
      " 37  state_CA                 1296675 non-null  int64  \n",
      " 38  state_CO                 1296675 non-null  int64  \n",
      " 39  state_CT                 1296675 non-null  int64  \n",
      " 40  state_DC                 1296675 non-null  int64  \n",
      " 41  state_DE                 1296675 non-null  int64  \n",
      " 42  state_FL                 1296675 non-null  int64  \n",
      " 43  state_GA                 1296675 non-null  int64  \n",
      " 44  state_HI                 1296675 non-null  int64  \n",
      " 45  state_IA                 1296675 non-null  int64  \n",
      " 46  state_ID                 1296675 non-null  int64  \n",
      " 47  state_IL                 1296675 non-null  int64  \n",
      " 48  state_IN                 1296675 non-null  int64  \n",
      " 49  state_KS                 1296675 non-null  int64  \n",
      " 50  state_KY                 1296675 non-null  int64  \n",
      " 51  state_LA                 1296675 non-null  int64  \n",
      " 52  state_MA                 1296675 non-null  int64  \n",
      " 53  state_MD                 1296675 non-null  int64  \n",
      " 54  state_ME                 1296675 non-null  int64  \n",
      " 55  state_MI                 1296675 non-null  int64  \n",
      " 56  state_MN                 1296675 non-null  int64  \n",
      " 57  state_MO                 1296675 non-null  int64  \n",
      " 58  state_MS                 1296675 non-null  int64  \n",
      " 59  state_MT                 1296675 non-null  int64  \n",
      " 60  state_NC                 1296675 non-null  int64  \n",
      " 61  state_ND                 1296675 non-null  int64  \n",
      " 62  state_NE                 1296675 non-null  int64  \n",
      " 63  state_NH                 1296675 non-null  int64  \n",
      " 64  state_NJ                 1296675 non-null  int64  \n",
      " 65  state_NM                 1296675 non-null  int64  \n",
      " 66  state_NV                 1296675 non-null  int64  \n",
      " 67  state_NY                 1296675 non-null  int64  \n",
      " 68  state_OH                 1296675 non-null  int64  \n",
      " 69  state_OK                 1296675 non-null  int64  \n",
      " 70  state_OR                 1296675 non-null  int64  \n",
      " 71  state_PA                 1296675 non-null  int64  \n",
      " 72  state_RI                 1296675 non-null  int64  \n",
      " 73  state_SC                 1296675 non-null  int64  \n",
      " 74  state_SD                 1296675 non-null  int64  \n",
      " 75  state_TN                 1296675 non-null  int64  \n",
      " 76  state_TX                 1296675 non-null  int64  \n",
      " 77  state_UT                 1296675 non-null  int64  \n",
      " 78  state_VA                 1296675 non-null  int64  \n",
      " 79  state_VT                 1296675 non-null  int64  \n",
      " 80  state_WA                 1296675 non-null  int64  \n",
      " 81  state_WI                 1296675 non-null  int64  \n",
      " 82  state_WV                 1296675 non-null  int64  \n",
      " 83  state_WY                 1296675 non-null  int64  \n",
      " 84  merchant_encoded         1296675 non-null  float64\n",
      " 85  merchant_freq            1296675 non-null  int64  \n",
      " 86  first_encoded            1296675 non-null  float64\n",
      " 87  first_freq               1296675 non-null  int64  \n",
      " 88  last_encoded             1296675 non-null  float64\n",
      " 89  last_freq                1296675 non-null  int64  \n",
      " 90  street_encoded           1296675 non-null  float64\n",
      " 91  street_freq              1296675 non-null  int64  \n",
      " 92  city_encoded             1296675 non-null  float64\n",
      " 93  city_freq                1296675 non-null  int64  \n",
      " 94  job_encoded              1296675 non-null  float64\n",
      " 95  job_freq                 1296675 non-null  int64  \n",
      "dtypes: float64(12), int32(9), int64(75)\n",
      "memory usage: 905.2 MB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Select the X and Y target"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:05.270983Z",
     "start_time": "2024-12-06T12:43:05.071954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X: pd.DataFrame = df_preprocessed[[col for col in df_preprocessed.columns if col != \"is_fraud\"]]\n",
    "y: pd.DataFrame = df_preprocessed[\"is_fraud\"]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:55:41.102249Z",
     "start_time": "2024-12-06T11:55:41.078968Z"
    }
   },
   "source": [
    "y.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_fraud\n",
       "0    1289169\n",
       "1       7506\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now oversample it using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:17.438536Z",
     "start_time": "2024-12-06T12:43:12.790344Z"
    }
   },
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "#Obtain the over sampled new values\n",
    "X_smote, y_smote = smote.fit_resample(X.astype(\"float\"), y)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:08:35.687751Z",
     "start_time": "2024-12-06T12:08:35.672247Z"
    }
   },
   "source": [
    "y_smote.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_fraud\n",
       "0    1289169\n",
       "1    1289169\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Select the continous columns:\n",
    "\n",
    "We will select the continous columns; where we are going to apply our `Standardscaler()` instance from scikit-learn to perform standardscaling on ONLY the continous features."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:18.591409Z",
     "start_time": "2024-12-06T12:43:18.059088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#List all the continous features\n",
    "continous_features = [\"cc_num\", \"amt\", \"zip\", \"lat\", \"long\", \"city_pop\", \"unix_time\",\n",
    "    \"merch_lat\", \"merch_long\", \"merch_zipcode\", \"transaction_year\", \"transaction_month\",\n",
    "    \"transaction_day\", \"transaction_hour\", \"transaction_minute\", \"transaction_second\",\n",
    "    \"birth_year\", \"birth_month\", \"birth_day\", \"merchant_encoded\", \"merchant_freq\",\n",
    "    \"first_encoded\", \"first_freq\", \"last_encoded\", \"last_freq\", \"street_encoded\",\n",
    "    \"street_freq\", \"city_encoded\", \"city_freq\", \"job_encoded\", \"job_freq\"]\n",
    "\n",
    "#Select the continous and not continous \n",
    "X_smote_continous = X_smote[continous_features]\n",
    "X_smote_discontinous = X_smote[[c for c in X_smote.columns if c not in continous_features]]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:20.914240Z",
     "start_time": "2024-12-06T12:43:20.892728Z"
    }
   },
   "cell_type": "code",
   "source": "X_smote_discontinous.head(3)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   gender  category_food_dining  category_gas_transport  category_grocery_net  \\\n",
       "0     1.0                   0.0                     0.0                   0.0   \n",
       "1     1.0                   0.0                     0.0                   0.0   \n",
       "2     0.0                   0.0                     0.0                   0.0   \n",
       "\n",
       "   category_grocery_pos  category_health_fitness  category_home  \\\n",
       "0                   0.0                      0.0            0.0   \n",
       "1                   1.0                      0.0            0.0   \n",
       "2                   0.0                      0.0            0.0   \n",
       "\n",
       "   category_kids_pets  category_misc_net  category_misc_pos  ...  state_SD  \\\n",
       "0                 0.0                1.0                0.0  ...       0.0   \n",
       "1                 0.0                0.0                0.0  ...       0.0   \n",
       "2                 0.0                0.0                0.0  ...       0.0   \n",
       "\n",
       "   state_TN  state_TX  state_UT  state_VA  state_VT  state_WA  state_WI  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       1.0       0.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   state_WV  state_WY  \n",
       "0       0.0       0.0  \n",
       "1       0.0       0.0  \n",
       "2       0.0       0.0  \n",
       "\n",
       "[3 rows x 64 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>category_food_dining</th>\n",
       "      <th>category_gas_transport</th>\n",
       "      <th>category_grocery_net</th>\n",
       "      <th>category_grocery_pos</th>\n",
       "      <th>category_health_fitness</th>\n",
       "      <th>category_home</th>\n",
       "      <th>category_kids_pets</th>\n",
       "      <th>category_misc_net</th>\n",
       "      <th>category_misc_pos</th>\n",
       "      <th>...</th>\n",
       "      <th>state_SD</th>\n",
       "      <th>state_TN</th>\n",
       "      <th>state_TX</th>\n",
       "      <th>state_UT</th>\n",
       "      <th>state_VA</th>\n",
       "      <th>state_VT</th>\n",
       "      <th>state_WA</th>\n",
       "      <th>state_WI</th>\n",
       "      <th>state_WV</th>\n",
       "      <th>state_WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 64 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implement the standardscaler to continous features"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:23.493550Z",
     "start_time": "2024-12-06T12:43:22.410141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Create a standard scaler object and fit x_train\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_smote_continous)\n",
    "\n",
    "#Transform x_train and x_test\n",
    "X_continous_scaled = standard_scaler.transform(X_smote_continous)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:24.793799Z",
     "start_time": "2024-12-06T12:43:24.781140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets make a pandas dataframe\n",
    "X_continous_scaled = pd.DataFrame(X_continous_scaled,\n",
    "    columns=X_smote_continous.columns)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:23:36.430023Z",
     "start_time": "2024-12-06T12:23:36.418022Z"
    }
   },
   "cell_type": "code",
   "source": "X_continous_scaled.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2578338, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:08:45.381891Z",
     "start_time": "2024-12-06T12:08:45.362879Z"
    }
   },
   "source": [
    "X_continous_scaled.head(3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     cc_num       amt       zip       lat      long  city_pop  unix_time  \\\n",
       "0 -0.314146 -0.849665 -0.730733 -0.493568  0.635088 -0.285761  -1.755668   \n",
       "1 -0.316236 -0.553411  1.873809  2.008165 -2.005238 -0.296425  -1.755666   \n",
       "2 -0.316207 -0.226389  1.286156  0.698217 -1.581124 -0.283660  -1.755666   \n",
       "\n",
       "   merch_lat  merch_long  merch_zipcode  ...  first_encoded  first_freq  \\\n",
       "0  -0.502642    0.572527      -0.748980  ...      -0.081514    1.395395   \n",
       "1   2.048246   -2.001306      -0.015091  ...      -0.201122    0.425288   \n",
       "2   0.882526   -1.571696       1.583852  ...      -0.274487   -0.747764   \n",
       "\n",
       "   last_encoded  last_freq  street_encoded  street_freq  city_encoded  \\\n",
       "0     -0.301663  -0.634622       -0.348284     0.578598     -0.335459   \n",
       "1     -0.227250  -0.472387       -0.348284     1.795535     -0.335459   \n",
       "2     -0.054028   0.176073       -0.348284    -1.273525     -0.335459   \n",
       "\n",
       "   city_freq  job_encoded  job_freq  \n",
       "0   0.201721    -0.255531 -0.059194  \n",
       "1   1.620086    -0.243193  0.686028  \n",
       "2  -1.224123     0.115153 -1.514150  \n",
       "\n",
       "[3 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amt</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>merch_zipcode</th>\n",
       "      <th>...</th>\n",
       "      <th>first_encoded</th>\n",
       "      <th>first_freq</th>\n",
       "      <th>last_encoded</th>\n",
       "      <th>last_freq</th>\n",
       "      <th>street_encoded</th>\n",
       "      <th>street_freq</th>\n",
       "      <th>city_encoded</th>\n",
       "      <th>city_freq</th>\n",
       "      <th>job_encoded</th>\n",
       "      <th>job_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.314146</td>\n",
       "      <td>-0.849665</td>\n",
       "      <td>-0.730733</td>\n",
       "      <td>-0.493568</td>\n",
       "      <td>0.635088</td>\n",
       "      <td>-0.285761</td>\n",
       "      <td>-1.755668</td>\n",
       "      <td>-0.502642</td>\n",
       "      <td>0.572527</td>\n",
       "      <td>-0.748980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>1.395395</td>\n",
       "      <td>-0.301663</td>\n",
       "      <td>-0.634622</td>\n",
       "      <td>-0.348284</td>\n",
       "      <td>0.578598</td>\n",
       "      <td>-0.335459</td>\n",
       "      <td>0.201721</td>\n",
       "      <td>-0.255531</td>\n",
       "      <td>-0.059194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.316236</td>\n",
       "      <td>-0.553411</td>\n",
       "      <td>1.873809</td>\n",
       "      <td>2.008165</td>\n",
       "      <td>-2.005238</td>\n",
       "      <td>-0.296425</td>\n",
       "      <td>-1.755666</td>\n",
       "      <td>2.048246</td>\n",
       "      <td>-2.001306</td>\n",
       "      <td>-0.015091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201122</td>\n",
       "      <td>0.425288</td>\n",
       "      <td>-0.227250</td>\n",
       "      <td>-0.472387</td>\n",
       "      <td>-0.348284</td>\n",
       "      <td>1.795535</td>\n",
       "      <td>-0.335459</td>\n",
       "      <td>1.620086</td>\n",
       "      <td>-0.243193</td>\n",
       "      <td>0.686028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.316207</td>\n",
       "      <td>-0.226389</td>\n",
       "      <td>1.286156</td>\n",
       "      <td>0.698217</td>\n",
       "      <td>-1.581124</td>\n",
       "      <td>-0.283660</td>\n",
       "      <td>-1.755666</td>\n",
       "      <td>0.882526</td>\n",
       "      <td>-1.571696</td>\n",
       "      <td>1.583852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274487</td>\n",
       "      <td>-0.747764</td>\n",
       "      <td>-0.054028</td>\n",
       "      <td>0.176073</td>\n",
       "      <td>-0.348284</td>\n",
       "      <td>-1.273525</td>\n",
       "      <td>-0.335459</td>\n",
       "      <td>-1.224123</td>\n",
       "      <td>0.115153</td>\n",
       "      <td>-1.514150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the actual datasets."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:43:28.443667Z",
     "start_time": "2024-12-06T12:43:27.526897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# These are the new datasets\n",
    "X_data = pd.concat([X_continous_scaled, X_smote_discontinous], axis=1)\n",
    "y_data = copy.copy(y_smote)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T11:56:00.657224Z",
     "start_time": "2024-12-06T11:56:00.640223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the shapes\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2578338, 95)\n",
      "(2578338,)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the following classifiers\n",
    "Now we will initialize each of the classifiers, and perform cross validation to obtain all the different metrics."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:45:13.707691Z",
     "start_time": "2024-12-06T12:45:13.691694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:56:56.455648Z",
     "start_time": "2024-12-06T12:56:56.439575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel:\n",
    "    def __init__(self, input_shape, epochs=20, batch_size=64, learning_rate=0.001, patience=2):\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.patience = patience\n",
    "\n",
    "        # Use GPU if available, otherwise fallback to CPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize the LSTM model\n",
    "        self.model = LSTMNet(input_size=input_shape[1], hidden_size=32, output_size=1).to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def prepare_data(self, X, y):\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.to_numpy()\n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        # Convert data to tensors (remain on CPU initially)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(2)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def fit(self, dataloader):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                # Move batch to GPU\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                # Zero gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(X_batch).squeeze()\n",
    "                loss = self.criterion(outputs, y_batch.squeeze())\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Print training loss for the epoch\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            # Clear GPU memory after each epoch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        # Convert data to tensor and move to GPU\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(2).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "        return (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the last hidden state for classification\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_hidden)\n",
    "        return self.sigmoid(output)\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:56:59.479125Z",
     "start_time": "2024-12-06T12:56:59.459076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_classifiers = {\n",
    "    # \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    # \"Decision Tree Classifier\": DecisionTreeClassifier(random_state=42),\n",
    "    # \"Linear Support Vector Machine\": LinearSVC(C=1.0, max_iter=1000),\n",
    "    \"LSTM Classifier\": LSTMModel(input_shape=(X_data.shape[1], 1), epochs=20, batch_size=128, learning_rate=0.001)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split the data\n",
    "We would like to perform the following:\n",
    "1. Split our data into train_validation and test; we would hold the test dataset for a final analysis.\n",
    "2. Perform cross validation using train_validation; and perform shuffled folds to see its accuracies.\n",
    "3. Keep the saved cross val scores; for later analysis."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:57:08.694445Z",
     "start_time": "2024-12-06T12:57:06.457981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtain the data into train_val and test\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X_data, y_data,\n",
    "    test_size=0.2, shuffle=True, random_state=42)\n",
    "print(x_train_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train_val.shape)\n",
    "print(y_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2062670, 95)\n",
      "(515668, 95)\n",
      "(2062670,)\n",
      "(515668,)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cross validation for all ML algorithms\n",
    "Perform cross validation for each of the ML algorithms, to obtain its results."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:07:28.879569Z",
     "start_time": "2024-12-06T12:59:06.920648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the holders for each metrics\n",
    "ml_metrics = {}\n",
    "\n",
    "for name, clf in ml_classifiers.items():\n",
    "    print(f\"\\n==========={name}============ Starting\")\n",
    "    # Initialize variables for each\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform 20 loops for all classifiers\n",
    "    for i in range(20):\n",
    "        print(f\"Iteration: {i} - {name}\")\n",
    "        # Call the train test split\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, shuffle=True)\n",
    "\n",
    "        # Handle LSTM separately\n",
    "        if name == \"LSTM Classifier\":\n",
    "            # Prepare the DataLoader for training\n",
    "            dataloader = clf.prepare_data(x_train, y_train)\n",
    "            clf.fit(dataloader)  # Removed val_data\n",
    "            y_pred = clf.predict(x_val)\n",
    "        else:\n",
    "            # Fit other classifiers\n",
    "            clf.fit(x_train, y_train)\n",
    "            y_pred = clf.predict(x_val)\n",
    "\n",
    "        # Append metrics\n",
    "        accuracies.append(accuracy_score(y_val, y_pred))\n",
    "        precisions.append(precision_score(y_val, y_pred))\n",
    "        recalls.append(recall_score(y_val, y_pred))\n",
    "        f1_scores.append(f1_score(y_val, y_pred))\n",
    "\n",
    "        # Clear GPU memory for LSTM\n",
    "        if name == \"LSTM Classifier\":\n",
    "            del clf.model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Add the metrics\n",
    "    ml_metrics[name + \"_accuracies\"] = accuracies\n",
    "    ml_metrics[name + \"_precisions\"] = precisions\n",
    "    ml_metrics[name + \"_recalls\"] = recalls\n",
    "    ml_metrics[name + \"_f1_scores\"] = f1_scores\n",
    "\n",
    "    # Create a dataframe and save it into a dataframe\n",
    "    df_results = pd.DataFrame(ml_metrics)\n",
    "    df_results.to_csv('my_data_' + name.replace(\" \", \"_\") + '.csv', index=False)\n",
    "    print(f\"\\n==========={name}============ Ending\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========LSTM Classifier============ Starting\n",
      "Iteration: 0 - LSTM Classifier\n",
      "Epoch 1/20, Training Loss: 8377.7847\n",
      "Epoch 2/20, Training Loss: 4591.8136\n",
      "Epoch 3/20, Training Loss: 3433.6091\n",
      "Epoch 4/20, Training Loss: 3099.3454\n",
      "Epoch 5/20, Training Loss: 3165.7602\n",
      "Epoch 6/20, Training Loss: 4940.2036\n",
      "Epoch 7/20, Training Loss: 6066.3642\n",
      "Epoch 8/20, Training Loss: 6230.3784\n",
      "Epoch 9/20, Training Loss: 4841.6575\n",
      "Epoch 10/20, Training Loss: 4032.4728\n",
      "Epoch 11/20, Training Loss: 4886.6333\n",
      "Epoch 12/20, Training Loss: 6584.0046\n",
      "Epoch 13/20, Training Loss: 6096.7374\n",
      "Epoch 14/20, Training Loss: 6260.2776\n",
      "Epoch 15/20, Training Loss: 6464.3083\n",
      "Epoch 16/20, Training Loss: 6781.3651\n",
      "Epoch 17/20, Training Loss: 7637.2723\n",
      "Epoch 18/20, Training Loss: 7669.5679\n",
      "Epoch 19/20, Training Loss: 8258.2390\n",
      "Epoch 20/20, Training Loss: 8276.2994\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 123.33 GiB. GPU 0 has a total capacity of 23.99 GiB of which 10.20 GiB is free. Of the allocated memory 7.21 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m     dataloader \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mprepare_data(x_train, y_train)\n\u001B[0;32m     22\u001B[0m     clf\u001B[38;5;241m.\u001B[39mfit(dataloader)  \u001B[38;5;66;03m# Removed val_data\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;66;03m# Fit other classifiers\u001B[39;00m\n\u001B[0;32m     26\u001B[0m     clf\u001B[38;5;241m.\u001B[39mfit(x_train, y_train)\n",
      "Cell \u001B[1;32mIn[28], line 72\u001B[0m, in \u001B[0;36mLSTMModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 72\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_tensor\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (predictions \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[28], line 86\u001B[0m, in \u001B[0;36mLSTMNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;66;03m# LSTM forward pass\u001B[39;00m\n\u001B[1;32m---> 86\u001B[0m     lstm_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;66;03m# Use the last hidden state for classification\u001B[39;00m\n\u001B[0;32m     88\u001B[0m     last_hidden \u001B[38;5;241m=\u001B[39m lstm_out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Argus\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1120\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1135\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1137\u001B[0m         batch_sizes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[0;32m   1145\u001B[0m     )\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 123.33 GiB. GPU 0 has a total capacity of 23.99 GiB of which 10.20 GiB is free. Of the allocated memory 7.21 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Now that we have finally got the cross validation metrics, we will perform the last steps in order to see the different results of each of the ML models. This would be illustrated as:\n",
    "\n",
    "1. Plot bar plot of accuracies, precision, recall, and f1 score for cross validation metrics of each of the models.\n",
    "\n",
    "2. Next; after having the cross validation metrics; we previously have isolated a part of the data as test set. We are going to train a final model for each ML classifier, and obtain the test metrics as well. With these test and cross validation metrics; we are going to perform the following:\n",
    "3. Plot bar plot of accuracies, precision, recall, and f1 score for test metrics of each of the models.\n",
    "4. Generate confusion matrices for each of the test final models.\n",
    "5. Create ROC curves for all models and overlay for comparison"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List\n",
    "def read_df_data(clfs_names:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that would retrieve the checkpoints cross validation\n",
    "    values for accuracy, precision, recall, and f1 score.\n",
    "    \"\"\"\n",
    "    #Create a holder to store the data\n",
    "    actual_data = {}\n",
    "\n",
    "    for name in clfs_names:\n",
    "        #Read the dataframe\n",
    "        df_data = pd.read_csv(\"my_data_\" + name + \".csv\")\n",
    "\n",
    "        #Assign it to the actual data properly\n",
    "        actual_data[name + \"_accuracies\"] = df_data[name + \"_accuracies\"].to_numpy()\n",
    "        actual_data[name + \"_precisions\"] = df_data[name + \"_precisions\"].to_numpy()\n",
    "        actual_data[name + \"_recalls\"] = df_data[name + \"_recalls\"].to_numpy()\n",
    "        actual_data[name + \"_f1_scores\"] = df_data[name + \"_f1_scores\"].to_numpy()\n",
    "    \n",
    "    #Return the total dataframe.\n",
    "    return pd.DataFrame(actual_data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Get the cross validation metrics\n",
    "df_cross_val_metrics = read_df_data(clfs_names=ml_classifiers.keys())\n",
    "\n",
    "print(ml_classifiers.keys())\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_cross_val_metrics.columns"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression_accuracies</th>\n",
       "      <th>Logistic Regression_precisions</th>\n",
       "      <th>Logistic Regression_recalls</th>\n",
       "      <th>Logistic Regression_f1_scores</th>\n",
       "      <th>Decition Tree Classifier_accuracies</th>\n",
       "      <th>Decition Tree Classifier_precisions</th>\n",
       "      <th>Decition Tree Classifier_recalls</th>\n",
       "      <th>Decition Tree Classifier_f1_scores</th>\n",
       "      <th>Linear Support Vector Machine_accuracies</th>\n",
       "      <th>Linear Support Vector Machine_precisions</th>\n",
       "      <th>Linear Support Vector Machine_recalls</th>\n",
       "      <th>Linear Support Vector Machine_f1_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899715</td>\n",
       "      <td>0.928023</td>\n",
       "      <td>0.866518</td>\n",
       "      <td>0.896217</td>\n",
       "      <td>0.998449</td>\n",
       "      <td>0.997649</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.998449</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>0.921700</td>\n",
       "      <td>0.842319</td>\n",
       "      <td>0.880223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.899538</td>\n",
       "      <td>0.927577</td>\n",
       "      <td>0.866385</td>\n",
       "      <td>0.895937</td>\n",
       "      <td>0.998449</td>\n",
       "      <td>0.997624</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.884928</td>\n",
       "      <td>0.919574</td>\n",
       "      <td>0.843096</td>\n",
       "      <td>0.879676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Logistic Regression_accuracies  Logistic Regression_precisions  \\\n",
       "0                        0.899715                        0.928023   \n",
       "1                        0.899538                        0.927577   \n",
       "\n",
       "   Logistic Regression_recalls  Logistic Regression_f1_scores  \\\n",
       "0                     0.866518                       0.896217   \n",
       "1                     0.866385                       0.895937   \n",
       "\n",
       "   Decition Tree Classifier_accuracies  Decition Tree Classifier_precisions  \\\n",
       "0                             0.998449                             0.997649   \n",
       "1                             0.998449                             0.997624   \n",
       "\n",
       "   Decition Tree Classifier_recalls  Decition Tree Classifier_f1_scores  \\\n",
       "0                          0.999251                            0.998449   \n",
       "1                          0.999284                            0.998453   \n",
       "\n",
       "   Linear Support Vector Machine_accuracies  \\\n",
       "0                                  0.885267   \n",
       "1                                  0.884928   \n",
       "\n",
       "   Linear Support Vector Machine_precisions  \\\n",
       "0                                  0.921700   \n",
       "1                                  0.919574   \n",
       "\n",
       "   Linear Support Vector Machine_recalls  \\\n",
       "0                               0.842319   \n",
       "1                               0.843096   \n",
       "\n",
       "   Linear Support Vector Machine_f1_scores  \n",
       "0                                 0.880223  \n",
       "1                                 0.879676  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_val_metrics.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Plot the cross validation bar plots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create two lists for a new dataframe, models and metrics\n",
    "models = ml_classifiers.keys()\n",
    "metrics = [\"accuracies\", \"precisions\", \"recalls\", \"f1_scores\"]\n",
    "\n",
    "#Lets do a df plot cross val\n",
    "plot_cross_val = []\n",
    "for model in models:\n",
    "    model_data = [df_cross_val_metrics[f\"{model}_{metric}\"].mean() for metric in metrics]\n",
    "    plot_cross_val.append(model_data)\n",
    "\n",
    "# Convert to a Pandas DataFrame for plotting\n",
    "df_plot_cross_val = pd.DataFrame(plot_cross_val, columns=metrics, index=models)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_plot_cross_val.head(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the bar graph of cross val metrics"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, figsize = (12, 6))\n",
    "df_plot_cross_val.T.plot(ax=axs, kind=\"bar\", legend=True, width = 0.8)\n",
    "\n",
    "# Rotate x-axis ticks\n",
    "axs.set_xticklabels(axs.get_xticklabels(), rotation=90, fontsize=10)\n",
    "\n",
    "# Annotate bar values\n",
    "for container in axs.containers:\n",
    "    axs.bar_label(container, fmt='%.4f', fontsize=10, padding=3)\n",
    "\n",
    "axs.set_ylim(0.83, 1.01)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train the final test models!\n",
    "Even thought its obvious the decision tree classifier is the best model between all the ML models; now we will perform the final test models and retrieve the metrics for each of them."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Create the holders for each metrics\n",
    "test_metrics = []\n",
    "conf_matrices = []\n",
    "class_reports = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "for name, clf in ml_classifiers.items():\n",
    "    print(f\"\\n==========={name}============ Starting\")\n",
    "\n",
    "    # Fit the model with x_train_val and the y_train_val dataset now!\n",
    "    clf.fit(X=x_train_val, y=y_train_val)\n",
    "\n",
    "    #Obtain the predictions for both\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Append them in order per row (accuracy, precision, recall, f1_score)\n",
    "    test_metrics.append(accuracy_score(y_test, y_pred))\n",
    "    test_metrics.append(precision_score(y_test, y_pred))\n",
    "    test_metrics.append(recall_score(y_test, y_pred))\n",
    "    test_metrics.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Append the confusion matrix\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    class_reports.append(classification_report(y_test, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\n==========={name}============ Ending\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(test_metrics)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert this to a numpy array\n",
    "import numpy as np\n",
    "np_test_metrics = np.array(test_metrics)\n",
    "np_test_metrics = np_test_metrics.reshape(3,4)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(np_test_metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               accuracies  precisions   recalls  f1_scores\n",
      "Logistic Regression              0.900360    0.929433  0.867051   0.897159\n",
      "Decition Tree Classifier         0.998751    0.998118  0.999393   0.998755\n",
      "Linear Support Vector Machine    0.886353    0.922740  0.843939   0.881582\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe of the model the test metrics\n",
    "df_test_results = pd.DataFrame(np_test_metrics, columns=metrics, index=models)\n",
    "print(df_test_results.head(4))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, figsize = (12, 6))\n",
    "df_test_results.T.plot(ax=axs, kind=\"bar\", legend=True, width = 0.8)\n",
    "\n",
    "# Rotate x-axis ticks\n",
    "axs.set_xticklabels(axs.get_xticklabels(), rotation=90, fontsize=10)\n",
    "\n",
    "# Annotate bar values\n",
    "for container in axs.containers:\n",
    "    axs.bar_label(container, fmt='%.4f', fontsize=10, padding=3)\n",
    "\n",
    "axs.set_ylim(0.83, 1.01)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Print each of the classification reports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i, name in enumerate(ml_classifiers):\n",
    "    print(f\"\\n==========={name}============ class report\")\n",
    "    print(class_reports[i])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lets lot the confusion matrices now"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "for i, name in enumerate(ml_classifiers):\n",
    "    cm = conf_matrices[i]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "    plt.suptitle(name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Finally, print the ROC auc scores"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i, name in enumerate(ml_classifiers):\n",
    "    print(f\"{name} ROC_AUC_Scores: {roc_auc_scores[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
